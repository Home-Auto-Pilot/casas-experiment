{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ed4ef6b-4b67-4c1a-9ecf-12b60e1b0dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d400dfcc-34d6-4e27-b2b7-a25271f637b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensors = torch.load('input_seq_tensors.pt', weights_only=True)\n",
    "act_dev_tensors = torch.load('act_dev_tensors.pt', weights_only=True)\n",
    "act_state_tensors = torch.load('act_state_tensors.pt', weights_only=True)\n",
    "act_timing_tensors = torch.load('act_timing_tensors.pt', weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f35ef0c3-27d9-4179-84c7-caa4375e57e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "438bba98-720a-497b-aa94-149804d6e8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskLSTM(nn.Module):\n",
    "    def __init__(self, input_size, num_classes, num_devices, hidden_size=128, num_lstm_layers=4):\n",
    "        super(MultiTaskLSTM, self).__init__()\n",
    "        # LSTM shared layer\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size, \n",
    "            hidden_size=hidden_size, \n",
    "            num_layers=num_lstm_layers, \n",
    "            batch_first=True, \n",
    "            dropout=0.2)\n",
    "        \n",
    "        # Timing prediction head (regression)\n",
    "        self.timing_pred_head = nn.Linear(hidden_size, 1)  # 1 output for time prediction\n",
    "        \n",
    "        # Action classification head (classification)\n",
    "        self.act_state_head = nn.Linear(hidden_size, num_classes)  # num_classes output for classification\n",
    "        \n",
    "        # Device classification head (classification)\n",
    "        self.act_device_head = nn.Linear(hidden_size, num_devices)  # num_devices output for device classification\n",
    "    \n",
    "    def forward(self, x, lengths):\n",
    "        # Process the packed sequence with LSTM\n",
    "        lstm_out, (h_n, c_n) = self.lstm(x)\n",
    "        lstm_out = lstm_out[range(len(lstm_out)), lengths - 1]\n",
    "        \n",
    "        # Timing and action predictions\n",
    "        timing_pred = self.timing_pred_head(lstm_out)\n",
    "        action_pred = self.act_state_head(lstm_out)\n",
    "        device_pred = self.act_device_head(lstm_out)\n",
    "        \n",
    "        return timing_pred, action_pred, device_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc497f23-2c1f-45c5-9812-b7808915975d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MultiTaskLSTM(\n",
       "  (lstm): LSTM(9, 128, num_layers=2, batch_first=True, dropout=0.2)\n",
       "  (timing_pred_head): Linear(in_features=128, out_features=1, bias=True)\n",
       "  (act_state_head): Linear(in_features=128, out_features=2, bias=True)\n",
       "  (act_device_head): Linear(in_features=128, out_features=213, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_size = 9  # Number of input features (sensor data + action history)\n",
    "hidden_size = 128  # Hidden units for LSTM\n",
    "num_lstm_layers = 2;\n",
    "num_classes = 2  # Number of action classes (on/off)\n",
    "num_devices = 213  # Number of different devices\n",
    "\n",
    "# Initialize model, loss functions, and optimizer\n",
    "model = MultiTaskLSTM(input_size=input_size, num_lstm_layers=num_lstm_layers, hidden_size=hidden_size, num_classes=num_classes, num_devices=num_devices)\n",
    "criterion_timing = nn.MSELoss()  # Loss for timing prediction (regression)\n",
    "criterion_action = nn.CrossEntropyLoss()  # Loss for action type prediction (classification)\n",
    "criterion_device = nn.CrossEntropyLoss()  # Loss for device prediction (classification)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'device: {device}')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e062ad26-5d6b-4d9b-9cba-b15e0b214a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train(model, train_data_loader, num_epochs=3):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "    \n",
    "        for batch_idx, (inputs, y_act_device, y_act_state, y_timing, lengths) in tqdm(enumerate(train_data_loader)):\n",
    "            inputs, y_device, y_action, y_timing, lengths = (\n",
    "                inputs.float().to(device), \n",
    "                y_act_device.long().to(device), \n",
    "                y_act_state.long().to(device), \n",
    "                y_timing.float().to(device), \n",
    "                lengths # this guy stays in CPU\n",
    "            )\n",
    "            \n",
    "            # Forward pass\n",
    "            timing_pred, action_pred, device_pred = model(inputs, lengths)\n",
    "\n",
    "            # # Squeeze the output if necessary to match target shape\n",
    "            # timing_pred = timing_pred.squeeze(1)\n",
    "            # action_pred = action_pred.squeeze(1)\n",
    "            # device_pred = device_pred.squeeze(1)\n",
    "        \n",
    "            # Calculate losses\n",
    "            action_loss = criterion_action(action_pred, y_action)\n",
    "            timing_loss = criterion_timing(timing_pred, y_timing)\n",
    "            device_loss = criterion_device(device_pred, y_device)\n",
    "\n",
    "            device_weight = 1.0  # Higher weight for more importance\n",
    "            action_weight = 1.0  # Higher weight for more importance\n",
    "            timing_weight = 0.5  # Lower weight for less importance\n",
    "            \n",
    "            # Calculate the weighted loss\n",
    "            weighted_loss = (device_weight * device_loss) + (action_weight * action_loss) + (timing_weight * timing_loss)\n",
    "\n",
    "            # Backpropagation and optimization\n",
    "            optimizer.zero_grad()\n",
    "            weighted_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {weighted_loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ab23822-ba7b-4a52-9713-45573fb452b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23414"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(act_dev_tensors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca1e8f3d-36e3-432c-9f5d-af2ef6d20d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of sequences: 23414, number of device/state/timing: 23414/23414/23414\n"
     ]
    }
   ],
   "source": [
    "print(f'number of sequences: {len(input_tensors)}, number of device/state/timing: {len(act_dev_tensors)}/{len(act_state_tensors)}/{len(act_timing_tensors)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8419e681-b0aa-45cd-8fee-5f1509fddefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "# Determine the sizes of each split\n",
    "total_size = len(input_tensors)\n",
    "train_size = int(0.7 * total_size)\n",
    "val_size = int(0.15 * total_size)\n",
    "test_size = total_size - train_size - val_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bae69336-276c-42f1-89b9-570a40e38b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class SmartHomeDataset(Dataset):\n",
    "    def __init__(self, input_tensors, act_dev_tensors, act_state_tensors, act_timing_tensors):\n",
    "        self.input_tensors = input_tensors\n",
    "        self.act_dev_tensors = act_dev_tensors\n",
    "        self.act_state_tensors = act_state_tensors\n",
    "        self.act_timing_tensors = act_timing_tensors\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_tensors)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.input_tensors[idx], \n",
    "                self.act_dev_tensors[idx], \n",
    "                self.act_state_tensors[idx], \n",
    "                self.act_timing_tensors[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c6c1b72-5c58-4855-845a-292ed5fdf2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # Separate inputs and targets\n",
    "    inputs, y_device, y_action, y_timing = zip(*batch)\n",
    "    \n",
    "    # Pad sequences for inputs (batch_first=True makes it [batch_size, seq_len, features])\n",
    "    inputs_padded = pad_sequence(inputs, batch_first=True, padding_value=0)\n",
    "    \n",
    "    # Convert targets to tensors (they should all have the same length as they're scalar values)\n",
    "    y_device = torch.stack(y_device)\n",
    "    y_action = torch.stack(y_action)\n",
    "    y_timing = torch.stack(y_timing)\n",
    "    \n",
    "    # Compute lengths for each sequence (before padding)\n",
    "    lengths = torch.tensor([len(seq) for seq in inputs])\n",
    "    \n",
    "    return inputs_padded, y_device, y_action, y_timing, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "33ab828f-012e-4b59-9033-d4b80092c5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SmartHomeDataset(input_tensors, act_dev_tensors, act_state_tensors, act_timing_tensors)\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb1e8d2d-4c1e-479a-9c5e-c32b646698cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 32  # Adjust as needed\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff450f4-42bc-4fd6-8a68-8388fbba9e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "513it [00:07, 67.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/8], Loss: 857.2107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "48it [00:00, 73.38it/s]"
     ]
    }
   ],
   "source": [
    "train(model, train_loader, num_epochs=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6f1d8fda-f592-4008-9f8e-e13a40b50bb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 484.5455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([24])) that is different to the input size (torch.Size([24, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "val_loss = 0.0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, y_device, y_action, y_timing, lengths in val_loader:\n",
    "        # Convert input tensors to float32\n",
    "        inputs = inputs.float().to(device)\n",
    "        \n",
    "        # Convert target tensors to appropriate types\n",
    "        y_device = y_device.long().to(device)    # Device classification targets should be long\n",
    "        y_action = y_action.long().to(device)    # Action classification targets should be long\n",
    "        y_timing = y_timing.float().to(device)   # Timing targets should remain float32\n",
    "        \n",
    "        lengths = lengths  # lengths can stay as integers and needs to be in CPU\n",
    "\n",
    "        # Forward pass through the model\n",
    "        timing_pred, action_pred, device_pred = model(inputs, lengths)\n",
    "        \n",
    "        # Calculate losses\n",
    "        loss_timing = criterion_timing(timing_pred, y_timing)\n",
    "        loss_action = criterion_action(action_pred, y_action)\n",
    "        loss_device = criterion_device(device_pred, y_device)\n",
    "        \n",
    "        val_loss += (loss_timing + loss_action + loss_device).item()\n",
    "\n",
    "val_loss /= len(val_loader)\n",
    "print(f'Validation Loss: {val_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "560f5091-97e2-42a0-90d9-a595caacccfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 451.7152\n",
      "Device Prediction Accuracy: 0.6129\n",
      "Action Prediction Accuracy: 0.5485\n"
     ]
    }
   ],
   "source": [
    "model.eval()  # Set the model to evaluation mode\n",
    "test_loss = 0.0\n",
    "correct_device = 0\n",
    "correct_action = 0\n",
    "total_samples = 0\n",
    "\n",
    "with torch.no_grad():  # No need to calculate gradients during testing\n",
    "    for inputs, y_device, y_action, y_timing, lengths in test_loader:\n",
    "        # Move data to the same device as the model\n",
    "        inputs = inputs.float().to(device)\n",
    "        y_device = y_device.long().to(device)\n",
    "        y_action = y_action.long().to(device)\n",
    "        y_timing = y_timing.float().to(device)\n",
    "        lengths = lengths.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        timing_pred, action_pred, device_pred = model(inputs, lengths)\n",
    "\n",
    "        # Squeeze the predictions if necessary\n",
    "        timing_pred = timing_pred.squeeze(1)\n",
    "        action_pred = action_pred.squeeze(1)\n",
    "        device_pred = device_pred.squeeze(1)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss_timing = criterion_timing(timing_pred, y_timing)\n",
    "        loss_action = criterion_action(action_pred, y_action)\n",
    "        loss_device = criterion_device(device_pred, y_device)\n",
    "\n",
    "        # Accumulate total loss\n",
    "        test_loss += (loss_timing + loss_action + loss_device).item()\n",
    "\n",
    "        # Calculate accuracy for device and action predictions\n",
    "        _, predicted_device = torch.max(device_pred, 1)\n",
    "        _, predicted_action = torch.max(action_pred, 1)\n",
    "        \n",
    "        correct_device += (predicted_device == y_device).sum().item()\n",
    "        correct_action += (predicted_action == y_action).sum().item()\n",
    "        total_samples += y_device.size(0)\n",
    "\n",
    "# Calculate average loss\n",
    "test_loss /= len(test_loader)\n",
    "\n",
    "# Calculate accuracy\n",
    "device_accuracy = correct_device / total_samples\n",
    "action_accuracy = correct_action / total_samples\n",
    "\n",
    "print(f'Test Loss: {test_loss:.4f}')\n",
    "print(f'Device Prediction Accuracy: {device_accuracy:.4f}')\n",
    "print(f'Action Prediction Accuracy: {action_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be273ab-54e3-48e1-b23a-cc7cb63f5bd4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
